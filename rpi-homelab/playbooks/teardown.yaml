# teardown.yaml
# https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tear-down
---
- name: Drain the worker nodes
  become: true
  gather_facts: no
  hosts: main
  tasks:
    - name: Drain worker node
      ansible.builtin.command: >-
        kubectl drain {{ hostvars[item]['ansible_host'].split(".")[0] }}
        --delete-emptydir-data --force --ignore-daemonsets
      with_items: "{{ groups['workers'] }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

- name: Reset and clean up the worker nodes
  become: true
  gather_facts: no
  hosts: workers
  tasks:
    - name: Run kubeadm reset
      ansible.builtin.command: kubeadm reset --force

    - name: Post-reset clean up
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/cni/net.d
        - /home/pi/.kube
        - /root/.kube

    - name: Reset and clean up iptables rules
      ansible.builtin.shell: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

- name: Delete the worker nodes
  become: true
  gather_facts: no
  hosts: main
  tasks:
    - name: Delete worker node
      ansible.builtin.command: kubectl delete node {{ hostvars[item]['ansible_host'].split(".")[0] }}
      with_items: "{{ groups['workers'] }}"
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf

- name: Reset and clean up the control plane
  become: true
  gather_facts: no
  hosts: main
  tags: [cp_only]
  tasks:
    - name: Run kubeadm reset
      ansible.builtin.shell: kubeadm reset --force

    - name: Post-reset clean up
      ansible.builtin.file:
        path: /etc/kubernetes
        state: absent

- name: Rolling reboot across all nodes
  gather_facts: no
  hosts: all
  serial: 1
  tasks:
    - name: Reboot
      become: true
      ansible.builtin.reboot:

    - name: Sleep for 10 seconds and continue with play
      wait_for:
        timeout: 10
      delegate_to: localhost
